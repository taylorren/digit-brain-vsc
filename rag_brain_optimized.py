import os
from dotenv import load_dotenv
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import requests
from functools import lru_cache
import time

load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))

# é…ç½® Ollama
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "qwen3:latest"

# é…ç½® Zhipu
ZHIPU_API_KEY = os.environ.get("ZHIPU_API_KEY")
ZHIPU_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
ZHIPU_MODEL = "glm-4-air"

# âš¡ Performance optimization: Load model and index only once
print("ğŸ”§ Loading models and indexes...")
start_time = time.time()

# Load the local paraphrase-multilingual-MiniLM-L12-v2 model for better Chinese support
model = SentenceTransformer(os.path.join(os.path.dirname(__file__), 'models', 'paraphrase-multilingual-MiniLM-L12-v2'))

# Load FAISS index and metadata
index = faiss.read_index(os.path.join(os.path.dirname(__file__), 'md_faiss.index'))
with open(os.path.join(os.path.dirname(__file__), 'md_faiss_meta.json'), 'r', encoding='utf-8') as f:
    meta = json.load(f)

print(f"âœ… Models loaded in {time.time() - start_time:.2f}s")

# âš¡ Performance optimization: Cache search results for repeated queries
@lru_cache(maxsize=100)
def search_cached(query, top_k=5):
    """Cached version of search for better performance"""
    return search_impl(query, top_k)

def search_impl(query, top_k=5):
    """Optimized search implementation"""
    # âš¡ Use batch encoding for potential future multi-query optimization
    query_vec = model.encode([query], show_progress_bar=False, convert_to_numpy=True)
    
    # âš¡ Direct numpy array without extra conversion
    D, I = index.search(query_vec.astype('float32'), top_k)
    
    # âš¡ Pre-allocate results list for better memory performance
    results = []
    
    for idx, score in zip(I[0], D[0]):
        results.append({
            'score': float(score),
            'path': meta[idx]['path'],
            'content': meta[idx]['content'][:800]  # å¢åŠ åˆ°800å­—ç¬¦è·å¾—æ›´å¤šä¸Šä¸‹æ–‡
        })
    
    return results

def search(query, top_k=5):
    """Public search interface with caching"""
    # Use the query string directly for caching
    return search_cached(query, top_k)

def rag_ask(query, top_k=10, use_zhipu=False):
    """
    RAGé—®ç­”å‡½æ•°ï¼Œå¸¦æœ‰æ”¹è¿›çš„æºå¼•ç”¨åŠŸèƒ½å’Œæ€§èƒ½ä¼˜åŒ–
    """
    start_time = time.time()
    print("[1/3] ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³ç‰‡æ®µ...")
    
    docs = search(query, top_k)
    search_time = time.time() - start_time
    print(f"[2/3] ğŸ“„ å·²æ£€ç´¢åˆ°{len(docs)}ä¸ªç‰‡æ®µï¼Œæ­£åœ¨ç»„ç»‡æç¤ºè¯... (æ£€ç´¢è€—æ—¶: {search_time:.2f}s)")
    
    # ç›¸ä¼¼åº¦åˆ†æå’Œè¿‡æ»¤
    best_score = docs[0]['score'] if docs else float('inf')
    worst_score = docs[-1]['score'] if docs else float('inf')
    
    # ç›¸ä¼¼åº¦è´¨é‡è¯„ä¼° (L2è·ç¦»ï¼šæ•°å€¼è¶Šå°è¶Šç›¸ä¼¼)
    if best_score > 12:
        print(f"âš ï¸  ç›¸ä¼¼åº¦è­¦å‘Šï¼šæœ€ä½³åŒ¹é…åº¦ä¸º {best_score:.3f}ï¼Œç›¸å…³æ€§è¾ƒä½")
    elif best_score < 5:
        print(f"âœ… é«˜è´¨é‡åŒ¹é…ï¼šæœ€ä½³ç›¸ä¼¼åº¦ {best_score:.3f}")
    
    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼ˆå®Œæ•´åˆ—è¡¨ï¼Œå·²æŒ‰ç›¸å…³åº¦æ’åºï¼‰
    print(f"\nğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼ˆå…±{len(docs)}ä¸ªï¼ŒæŒ‰ç›¸å…³åº¦æ’åºï¼Œç›¸ä¼¼åº¦èŒƒå›´: {best_score:.3f} - {worst_score:.3f}ï¼‰:")
    for i, d in enumerate(docs, 1):
        file_name = os.path.basename(d['path'])
        file_ext = os.path.splitext(file_name)[1].upper()
        # æ·»åŠ æ–‡ä»¶ç±»å‹å›¾æ ‡
        if file_ext == '.MD':
            icon = "ğŸ“„"
        elif file_ext == '.PPTX':
            icon = "ğŸ¯"  # ä½¿ç”¨ç‰¹æ®Šå›¾æ ‡çªå‡ºPPTæ–‡ä»¶
        elif file_ext == '.PDF':
            icon = "ğŸ“‹"
        else:
            icon = "ğŸ“"
        
        # ç›¸ä¼¼åº¦é¢œè‰²æ ‡è¯† (L2è·ç¦»ï¼šè¶Šå°è¶Šç›¸ä¼¼)
        if d['score'] < 8:
            score_indicator = "ğŸŸ¢"  # é«˜ç›¸å…³ (è·ç¦»å°)
        elif d['score'] < 12:
            score_indicator = "ğŸŸ¡"  # ä¸­ç­‰ç›¸å…³
        else:
            score_indicator = "ğŸ”´"  # ä½ç›¸å…³ (è·ç¦»å¤§)
        
        # æ·»åŠ æ’åæŒ‡ç¤º
        rank_indicator = f"#{i}" if i <= 3 else f"#{i}"
        print(f"  {rank_indicator} {icon} {file_name} {score_indicator} (ç›¸ä¼¼åº¦: {d['score']:.3f})")
    print("")  # ç©ºè¡Œåˆ†éš”
    
    # æ„å»ºä¸Šä¸‹æ–‡ï¼Œæ˜ç¡®æ ‡æ³¨æ¯ä¸ªç‰‡æ®µçš„æ¥æº
    # æ™ºèƒ½è¿‡æ»¤ï¼šå¦‚æœæœ€ä½³åŒ¹é…åº¦å¤ªé«˜(è·ç¦»å¤§=ä¸ç›¸ä¼¼)ï¼Œå‡å°‘ä½¿ç”¨çš„æ–‡æ¡£æ•°é‡
    if best_score > 15:
        filtered_docs = docs[:3]  # ç›¸å…³æ€§å¾ˆä½ï¼Œåªä½¿ç”¨å‰3ä¸ª
        print(f"ğŸ” ç”±äºç›¸å…³æ€§è¾ƒä½ï¼Œåªä½¿ç”¨å‰{len(filtered_docs)}ä¸ªæœ€ç›¸å…³æ–‡æ¡£")
    elif best_score > 10:
        filtered_docs = docs[:6]  # ç›¸å…³æ€§ä¸­ç­‰ï¼Œä½¿ç”¨å‰6ä¸ª
    else:
        filtered_docs = docs  # ç›¸å…³æ€§é«˜ï¼Œä½¿ç”¨å…¨éƒ¨
    
    # âš¡ Performance optimization: Use list comprehension instead of loop
    context_parts = [
        f"ã€ç‰‡æ®µ{i}ã€‘\næ¥æºæ–‡ä»¶ï¼š{os.path.basename(d['path'])}\nå†…å®¹ï¼š{d['content']}"
        for i, d in enumerate(filtered_docs, 1)
    ]
    context = "\n\n".join(context_parts)
    
    # å¼ºåŒ–ç‰ˆæç¤ºè¯ï¼Œé‡ç‚¹å¼ºè°ƒå¼•ç”¨è¦æ±‚å’Œæ–‡ä»¶ç±»å‹
    prompt = (
        "ä½ æ˜¯ä»»è€å¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹èµ„æ–™ç‰‡æ®µå›ç­”é—®é¢˜ã€‚\n\n"
        "**ä¸¥æ ¼è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰:**\n"
        "1. ä½¿ç”¨'ä»»è€å¸ˆè®¤ä¸º'ã€'ä»»è€å¸ˆçš„è§‚ç‚¹æ˜¯'ç­‰è¡¨è¾¾æ–¹å¼\n"
        "2. æ¯ä¸ªè§‚ç‚¹å**å¿…é¡»**ç”¨æ–¹æ‹¬å·æ ‡æ³¨æ¥æºï¼Œæ ¼å¼ï¼š[æ¥æºï¼šæ–‡ä»¶å.æ‰©å±•å]\n"
        "3. æ³¨æ„èµ„æ–™åŒ…å«å¤šç§æ ¼å¼ï¼š.mdï¼ˆMarkdownï¼‰ã€.pptxï¼ˆPowerPointï¼‰ã€.pdfï¼ˆPDFï¼‰\n"
        "4. **ä¼˜å…ˆå¼•ç”¨ç›¸å…³åº¦æœ€é«˜çš„ç‰‡æ®µ**ï¼Œæ— è®ºæ˜¯ä»€ä¹ˆæ–‡ä»¶æ ¼å¼\n"
        "5. ä¸è¦è¾“å‡º<think>æ ‡è®°\n"
        "6. ç¡®ä¿æ¯ä¸ªè¦ç‚¹éƒ½æœ‰æ˜ç¡®çš„æ¥æºæ ‡æ³¨\n\n"
        "èµ„æ–™ç‰‡æ®µï¼š\n"
        f"{context}\n\n"
        f"é—®é¢˜ï¼š{query}\n\n"
        "è¯·æŒ‰è¦æ±‚å›ç­”ï¼Œæ¯ä¸ªè§‚ç‚¹éƒ½è¦æ ‡æ³¨æ¥æºï¼Œç‰¹åˆ«æ³¨æ„å¼•ç”¨PPTå’ŒPDFæ–‡ä»¶å†…å®¹ï¼š"
    )
    
    if use_zhipu:
        return _call_zhipu(prompt)
    else:
        return _call_ollama(prompt, docs)


def _call_zhipu(prompt):
    """è°ƒç”¨æ™ºè°±AI - ä¼˜åŒ–ç‰ˆ"""
    if not ZHIPU_API_KEY:
        return "[Zhipu API Key æœªè®¾ç½®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ZHIPU_API_KEY]"
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {ZHIPU_API_KEY}"
    }
    data = {
        "model": ZHIPU_MODEL,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.6,
        "stream": False
    }
    try:
        ai_start = time.time()
        print("[3/3] ğŸ¤– æ­£åœ¨ç­‰å¾…æ™ºè°±AIç”Ÿæˆå›ç­”...")
        resp = requests.post(ZHIPU_URL, headers=headers, json=data, timeout=120)
        resp.raise_for_status()
        result = resp.json()
        ai_time = time.time() - ai_start
        print(f"ğŸ¤– AIå›ç­”ç”Ÿæˆè€—æ—¶: {ai_time:.2f}s")
        
        content = result.get("choices", [{}])[0].get("message", {}).get("content", "[æ™ºè°±æœªè¿”å›å†…å®¹]")
        return content.strip()
    except Exception as e:
        return f"[æ™ºè°±è°ƒç”¨å¤±è´¥: {e}]"


def _call_ollama(prompt, docs):
    """è°ƒç”¨Ollamaæœ¬åœ°æ¨¡å‹ - ä¼˜åŒ–ç‰ˆ"""
    payload = {
        "model": OLLAMA_MODEL,
        "prompt": prompt,
        "stream": False
    }
    try:
        ai_start = time.time()
        print("[3/3] ğŸ¤– æ­£åœ¨ç­‰å¾…Ollamaç”Ÿæˆå›ç­”...")
        resp = requests.post(OLLAMA_URL, json=payload, timeout=120)
        resp.raise_for_status()
        result = resp.json()
        ai_time = time.time() - ai_start
        print(f"ğŸ¤– AIå›ç­”ç”Ÿæˆè€—æ—¶: {ai_time:.2f}s")
        
        response = result.get("response", "[Ollamaæœªè¿”å›å†…å®¹]")
        
        # æ¸…ç†å“åº”
        import re
        response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¥æºæ ‡æ³¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨æ·»åŠ 
        if not re.search(r'\[æ¥æºï¼š.*?\]', response) and not re.search(r'\(.*\.md\)', response):
            print("âš ï¸  æ£€æµ‹åˆ°å›ç­”ç¼ºå°‘æ¥æºæ ‡æ³¨ï¼Œè‡ªåŠ¨æ·»åŠ ...")
            source_list = [os.path.basename(d['path']) for d in docs[:3]]
            response += f"\n\n**å‚è€ƒæ¥æº:** {', '.join(source_list)}"
        
        return response.strip()
    except Exception as e:
        return f"[Ollamaè°ƒç”¨å¤±è´¥: {e}]"


if __name__ == '__main__':
    use_zhipu = False
    print("\nğŸ§  æ•°å­—å¤§è„‘ - é«˜æ€§èƒ½RAGé—®ç­”ç³»ç»Ÿ âš¡")
    print("âœ¨ æ–°åŠŸèƒ½ï¼šæ€§èƒ½ä¼˜åŒ–ã€ç¼“å­˜ã€æ—¶é—´ç›‘æ§")
    print("\nå‘½ä»¤è¯´æ˜:")
    print("  zhipu  - åˆ‡æ¢åˆ°æ™ºè°±AI")
    print("  ollama - åˆ‡æ¢åˆ°Ollamaæœ¬åœ°æ¨¡å‹")
    print("  clear  - æ¸…ç©ºæœç´¢ç¼“å­˜")
    print("  exit   - é€€å‡ºç¨‹åº")
    
    while True:
        query = input('\nğŸ’­ è¯·è¾“å…¥ä½ çš„é—®é¢˜: ')
        if query.lower() == 'exit':
            print("ğŸ‘‹ å†è§ï¼")
            break
        if query.lower() == 'zhipu':
            use_zhipu = True
            print("âœ… å·²åˆ‡æ¢åˆ°æ™ºè°±AIæ¨¡å¼")
            continue
        if query.lower() == 'ollama':
            use_zhipu = False
            print("âœ… å·²åˆ‡æ¢åˆ°Ollamaæœ¬åœ°æ¨¡å¼")
            continue
        if query.lower() == 'clear':
            search_cached.cache_clear()
            print("ğŸ—‘ï¸ æœç´¢ç¼“å­˜å·²æ¸…ç©º")
            continue
            
        if query.strip():
            total_start = time.time()
            print('\nğŸš€ å¼€å§‹å¤„ç†...')
            answer = rag_ask(query, use_zhipu=use_zhipu)
            total_time = time.time() - total_start
            print(f'\nğŸ“ ã€ä»»è€å¸ˆçš„å›ç­”ã€‘\n{answer}\n')
            print(f"âš¡ æ€»è€—æ—¶: {total_time:.2f}s")
            print("-" * 50)